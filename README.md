#  OBS MODULO 10 - *Speech and Text Analytics*
![Speech to Text](https://ibb.co/JjXyb9Yr)

Este repositorio contiene el c贸digo que hemos desarrollado durante el m谩ster de IA en OBS para:

El proyecto se realiza en 4 etapas.
El c贸digo fuer realizado en python 3.12.10, utilizando visual studio code, el cual puede ser personalizado seg煤n se requiera.
La interfaz gr谩fica se ha realizado en GRADIO.

#  Aplicaci贸n Web de Transcripci贸n de Voz a Texto basada en Whisper

## 1. Introducci贸n
Este documento describe el dise帽o, implementaci贸n y funcionamiento de una aplicaci贸n web para la transcripci贸n autom谩tica de voz a texto. La aplicaci贸n ha sido desarrollada como parte del Trabajo Fin de M谩ster (TFM) en el contexto de la asignatura *Speech and Text Analytics*.
El objetivo principal del sistema es permitir al usuario convertir audio en texto de manera sencilla, ya sea grabando directamente desde un micr贸fono o subiendo un archivo de audio previamente grabado. Para ello, se utiliza el modelo Whisper de OpenAI, reconocido por su alto rendimiento en tareas de reconocimiento autom谩tico del habla (ASR, *Automatic Speech Recognition*).

---

## 2. Objetivos del sistema
Los objetivos principales de la aplicaci贸n son:

- Implementar un sistema de transcripci贸n de voz a texto basado en modelos de *Deep Learning*.
- Proporcionar una interfaz web intuitiva y accesible.
- Permitir la entrada de audio tanto en tiempo real (micr贸fono) como mediante archivos.
- Almacenar de forma autom谩tica las grabaciones procesadas.
- Garantizar un dise帽o visual claro y consistente.

---

## 3.  Tecnolog铆as empleadas

### 3.1 Python
Lenguaje principal de desarrollo, elegido por su ecosistema de librer铆as orientadas a *Machine Learning* y procesamiento de audio.

### 3.2 Gradio
Framework utilizado para la creaci贸n de la interfaz web. Permite conectar funciones de Python con componentes visuales de manera r谩pida y eficiente.

### 3.3 Whisper (OpenAI / Hugging Face)
Modelo de reconocimiento de voz a texto basado en *Transformers*. Se emplean dos posibles implementaciones:
- Whisper a trav茅s de la librer铆a `transformers` de Hugging Face.
- Whisper oficial de OpenAI como mecanismo de respaldo.

### 3.4 PyTorch
Se utiliza para detectar la disponibilidad de GPU (CUDA) y acelerar el proceso de inferencia cuando es posible.

### 3.5 FFmpeg
Herramienta empleada para normalizar los archivos de audio, asegurando un formato est谩ndar (mono, 16 kHz) adecuado para el reconocimiento del habla.

---

## 4. Arquitectura del sistema

La aplicaci贸n sigue una arquitectura sencilla de tipo cliente-servidor:

1. **Interfaz de usuario (UI)**: construida con Gradio y accesible desde un navegador web.
2. **L贸gica de negocio**: funci贸n de transcripci贸n que gestiona la entrada de audio, la inferencia del modelo y el almacenamiento.
3. **Modelo de ASR**: Whisper, encargado de transformar el audio en texto.
4. **Sistema de archivos**: utilizado para guardar las grabaciones procesadas.

---

## 5. Funcionamiento de la aplicaci贸n

### 5.1 Selecci贸n del dispositivo
Al iniciar la aplicaci贸n, se detecta autom谩ticamente si existe una GPU compatible con CUDA. En caso afirmativo, el modelo se ejecuta sobre GPU; de lo contrario, se utiliza la CPU.

### 5.2 Carga del modelo
Se intenta cargar el modelo Whisper mediante la librer铆a `transformers`. Si esta carga falla, se utiliza la implementaci贸n oficial de OpenAI Whisper como alternativa.

### 5.3 Entrada de audio
El usuario puede proporcionar audio de dos formas:

- Grabaci贸n directa desde el micr贸fono.
- Subida de un archivo de audio en formatos comunes (.wav, .mp3, .m4a, .flac, .ogg, .aac).

### 5.4 Proceso de transcripci贸n
Una vez recibido el audio:

1. Se genera un nombre de archivo 煤nico mediante un *timestamp*.
2. El audio se transcribe utilizando Whisper.
3. El archivo se normaliza y se guarda en el directorio local de grabaciones.
4. El texto transcrito se muestra en la interfaz web.

### 5.5 Salida
La aplicaci贸n devuelve:

- El texto transcrito.
- La ruta completa donde se ha almacenado el archivo de audio procesado.

---

## 6. Dise帽o de la interfaz

La interfaz se ha dise帽ado con los siguientes criterios:

- Dos columnas de ancho fijo, centradas en la pantalla.
- Separaci贸n clara entre entradas y salidas.
- Bot贸n principal de acci贸n que ocupa el ancho completo de ambas columnas.
- Adaptaci贸n a pantallas peque帽as mediante reglas *responsive* en CSS.

Este dise帽o mejora la usabilidad y facilita la comprensi贸n del flujo de interacci贸n.

---

## 7. Gesti贸n de errores

El sistema incluye mecanismos b谩sicos de control de errores:

- Validaci贸n de la existencia de audio antes de la transcripci贸n.
- Captura de excepciones durante la inferencia y el procesamiento de audio.
- Mensajes informativos mostrados al usuario en caso de fallo.

---

## 8.  Conclusiones

La aplicaci贸n desarrollada cumple los objetivos planteados, ofreciendo una soluci贸n funcional y accesible para la transcripci贸n autom谩tica de voz a texto. El uso de Whisper garantiza una alta calidad en el reconocimiento del habla, mientras que Gradio facilita la creaci贸n de una interfaz clara y eficiente.

Este sistema puede ampliarse en trabajos futuros incorporando caracter铆sticas como detecci贸n autom谩tica de idioma, diarizaci贸n de hablantes, transcripci贸n por segmentos con marcas temporales o integraci贸n con bases de datos.

---

## 9. Trabajo futuro

Como posibles l铆neas de mejora se proponen:

- A帽adir selecci贸n manual o detecci贸n autom谩tica del idioma.
- Incorporar marcas de tiempo por frase o palabra.
- Implementar procesamiento por lotes de m煤ltiples archivos.
- Integrar m茅tricas de evaluaci贸n del reconocimiento.
- Desplegar la aplicaci贸n en un entorno cloud.

---

## 10. Referencias

- OpenAI. *Whisper: Robust Speech Recognition via Large-Scale Weak Supervision*.
- Hugging Face Transformers Documentation.
- Gradio Documentation.
- FFmpeg Documentation.

---

## Los participantes de este proyecto hemos sido:
- Carlos
- Juan
- Nicolas
- Riccardo
